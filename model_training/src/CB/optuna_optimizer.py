import os
import optuna
import yaml
from math import sqrt
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, recall_score
from catboost import CatBoostRegressor, Pool
from xgboost import XGBRegressor
from src.utils import recall_at_k, load_true_matches


def compute_recall_at_k(y_true, y_pred, k=10):
    """
    Recall@K ê³„ì‚° í•¨ìˆ˜
    y_true: ì‹¤ì œ ë§¤ì¹­ëœ freelancer_id ëª©ë¡ (Set)
    y_pred: ì˜ˆì¸¡ëœ freelancer_id ëª©ë¡ (Top-K)
    """
    recall_scores = []
    
    for true, pred in zip(y_true, y_pred):
        true_set = set(true)  # ì‹¤ì œ ë§¤ì¹­ëœ freelancer_id
        pred_set = set(pred[:k])  # ì˜ˆì¸¡ëœ Top-K freelancer_id
        recall = len(true_set & pred_set) / len(true_set)  # TP / (TP + FN)
        recall_scores.append(recall)
    
    return np.mean(recall_scores)  # í‰ê·  Recall@K

class OptunaOptimizer:
    def __init__(self, config, model_type="catboost", n_trials=5):
        self.config = config
        self.model_type = model_type.lower()
        self.n_trials = n_trials

    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        train_path = os.path.join(self.config.data_path, "train.csv")
        test_path = os.path.join(self.config.data_path, "test.csv")

        train_data = pd.read_csv(train_path)
        test_data = pd.read_csv(test_path)

        return train_data, test_data

    def prepare_data(self, train_data, test_data):
        """Train/Test ë°ì´í„°ì—ì„œ Featureì™€ Targetì„ ë¶„ë¦¬ (Categorical Features ì œì™¸)"""
        numerical_features = self.config.data_params["numerical_features"]
        categorical_features = self.config.data_params["categorical_features"]
        target_column = self.config.data_params["target_column"]

        features = numerical_features + categorical_features
        
        X_train = train_data[features]  # Featureë§Œ ì„ íƒ
        y_train = train_data[target_column]  # Target (matching_score)
        X_test = test_data[features]
        y_test = test_data[target_column]

        return X_train, X_test, y_train, y_test, categorical_features

    def objective(self, trial):
        """Optuna ìµœì í™” ëª©í‘œ í•¨ìˆ˜"""
        train_data, test_data = self.load_data()
        X_train, X_test, y_train, y_test, categorical_features = self.prepare_data(train_data, test_data)
        
        if self.model_type == "catboost":
            params = {
                "iterations": trial.suggest_int("iterations", 100, 1000),
                "depth": trial.suggest_int("depth", 4, 10),
                "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.3),
                "l2_leaf_reg": trial.suggest_loguniform("l2_leaf_reg", 1e-3, 10),
                "random_seed": 42,
                "verbose": 0,
                "od_type": "Iter",  # ì¡°ê¸° ì¢…ë£Œ ì˜µì…˜ ì¶”ê°€
                "od_wait": 50,  # 50ë²ˆ ì´ìƒ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ
                "task_type": "GPU"
            }
            train_pool = Pool(X_train, y_train, cat_features=categorical_features)
            model = CatBoostRegressor(**params)
            model.fit(train_pool, early_stopping_rounds=50)

        elif self.model_type == "xgboost":
            params = {
                "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
                "max_depth": trial.suggest_int("max_depth", 3, 10),
                "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.3),
                "subsample": trial.suggest_uniform("subsample", 0.5, 1.0),
                "colsample_bytree": trial.suggest_uniform("colsample_bytree", 0.5, 1.0),
                "reg_lambda": trial.suggest_loguniform("reg_lambda", 1e-3, 10),
                "random_state": 42
            }
            model = XGBRegressor(**params)
            model.fit(X_train, y_train)

        else:
            raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. 'catboost' ë˜ëŠ” 'xgboost'ë¥¼ ì„ íƒí•˜ì„¸ìš”.")

        # ì˜ˆì¸¡ê°’ ê°€ì ¸ì˜¤ê¸°
        y_pred_scores = model.predict(X_test)

        # ğŸ”¥ í•´ê²° ë°©ë²• ì ìš©: 1ì°¨ì› ë°°ì—´ â†’ 2ì°¨ì› ë³€í™˜ í›„ argsort ìˆ˜í–‰
        y_pred_scores = np.array(y_pred_scores).reshape(-1, 1)  # (N,) â†’ (N,1) ë³€í™˜
        y_pred_top10 = np.argsort(-y_pred_scores, axis=0)[:10]  # âœ… ìˆ˜ì •ëœ ì •ë ¬ ì½”ë“œ

        # Recall@10 ê³„ì‚°
        recall_at_10 = compute_recall_at_k(y_test, y_pred_top10, k=10)

        return recall_at_10  # Optunaê°€ Recall@10 ê°’ì„ ìµœëŒ€í™”í•˜ë„ë¡ ì„¤ì •

    def run(self):
        """Optuna ìµœì í™” ì‹¤í–‰"""
        print(f"Optunaë¥¼ ì´ìš©í•œ {self.model_type} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...")
        study = optuna.create_study(direction="maximize")
        study.optimize(self.objective, n_trials=self.n_trials)

        print(f"âœ… ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {study.best_params}")
        self.save_best_params(study.best_params)

    def save_best_params(self, best_params):
        """config.yamlì˜ xgboost/catboost ë¶€ë¶„ë§Œ ì—…ë°ì´íŠ¸"""
        config_path = "config/config.yaml"  # ê¸°ì¡´ config íŒŒì¼ ê²½ë¡œ

        # 1ï¸âƒ£ ê¸°ì¡´ YAML íŒŒì¼ ë¡œë“œ
        if os.path.exists(config_path):
            with open(config_path, "r") as f:
                config_data = yaml.safe_load(f)  # ê¸°ì¡´ ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
        else:
            config_data = {}  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ìƒì„±

        # 2ï¸âƒ£ catboost_params ë¶€ë¶„ ì—…ë°ì´íŠ¸
        if f"{self.model_type}_params" not in config_data:
            config_data["catboost_params"] = {}

        config_data[f"{self.model_type}_params"].update(best_params)

        # 3ï¸âƒ£ ì—…ë°ì´íŠ¸ëœ ì„¤ì •ì„ ë‹¤ì‹œ ì €ì¥
        with open(config_path, "w") as f:
            yaml.dump(config_data, f, default_flow_style=False, sort_keys=False)

        print(f"ğŸ“¢ {self.model_type}_params ì—…ë°ì´íŠ¸ ì™„ë£Œ: {config_path}")