import os
import numpy as np
import pandas as pd
import torch

from tqdm import tqdm
from typing import Optional, Tuple, Union

from sqlalchemy import text
from api.db import SessionLocal
from src.utils import check_path
from src.preprocessing import Preprocessing


def load_data(data_path: str):
    """
    DBÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÌõÑ CSV ÌååÏùºÎ°ú Ï†ÄÏû•

    Args:
        data_path (str): Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Í≤ΩÎ°ú
    """
    db = SessionLocal()

    try:
        sql_project_info = """
            SELECT  P.PROJECT_ID AS project_id,
                    P.DURATION AS duration,
                    ROUND(P.BUDGET / P.DURATION, 0) AS budget,
                    P.PRIORITY AS priority,
                    P.COMPANY_ID AS company_id,
                    (SELECT JSON_ARRAYAGG(PC.CATEGORY_ID)
                     FROM PROJECT PC
                     WHERE PC.PROJECT_ID = P.PROJECT_ID) AS category_id,
                    (SELECT JSON_ARRAYAGG(PS.SKILL_ID)
                     FROM PROJECT_SKILL PS
                     WHERE PS.PROJECT_ID = P.PROJECT_ID) AS skill_id
            FROM    PROJECT P
            JOIN    CATEGORY C ON P.CATEGORY_ID = C.CATEGORY_ID
            WHERE   P.STATUS IN (1, 2)
            """

        sql_project_content = """
            SELECT PROJECT_ID AS project_id,
                   DBMS_LOB.SUBSTR(PROJECT_CONTENT, 32767, 1) AS project_content
            FROM PROJECT
            WHERE STATUS IN (1, 2)
            """

        sql_freelancer = """
            SELECT  F.FREELANCER_ID AS freelancer_id,
                    F.WORK_EXP AS work_exp,
                    ROUND(F.PRICE / 365, 0) AS price,
                    (SELECT JSON_ARRAYAGG(FS.SKILL_ID) AS SKILL_ID
                     FROM FREELANCER_SKILL FS
                     WHERE FS.FREELANCER_ID = F.FREELANCER_ID) AS skill_id,
                    (SELECT JSON_ARRAYAGG(FS.SKILL_SCORE) AS SKILL_TEMP
                     FROM FREELANCER_SKILL FS
                     WHERE FS.FREELANCER_ID = F.FREELANCER_ID) AS skill_temp,
                    (SELECT JSON_ARRAYAGG(FC.CATEGORY_ID)
                     FROM FREELANCER_CATEGORY FC
                     WHERE FC.FREELANCER_ID = F.FREELANCER_ID) AS category_id
            FROM    FREELANCER F
            """

        sql_inter = """
            SELECT  PROJECT_ID AS project_id,
                    FREELANCER_ID AS freelancer_id,
                    MATCHING_SCORE AS matching_score
            FROM (
                SELECT  PROJECT_ID,
                        FREELANCER_ID,
                        MATCHING_SCORE / 100 AS MATCHING_SCORE,
                        ROW_NUMBER() OVER (PARTITION BY PROJECT_ID ORDER BY MATCHING_SCORE DESC) AS RANKING
                FROM    PROJECT_RANKING
            )
            ORDER BY PROJECT_ID, RANKING
            """

        sql_inter_implicit = """
            SELECT  PROJECT_ID AS project_id,
                    FREELANCER_ID AS freelancer_id,
                    (CASE WHEN RANKING <= 10 THEN 1
                          ELSE 0
                    END) AS matching_score
            FROM (
                SELECT  PROJECT_ID,
                        FREELANCER_ID,
                        ROW_NUMBER() OVER (PARTITION BY PROJECT_ID ORDER BY MATCHING_SCORE DESC) AS RANKING
                FROM    PROJECT_RANKING
            )
            ORDER BY PROJECT_ID, RANKING
            """

        project_info_df = pd.read_sql(text(sql_project_info), db.bind)
        project_content_df = pd.read_sql(text(sql_project_content), db.bind)
        project_df = project_info_df.merge(project_content_df, on="project_id", how="left")
        freelancer_df = pd.read_sql(text(sql_freelancer), db.bind)
        inter_df = pd.read_sql(text(sql_inter), db.bind)
        inter_implicit_df = pd.read_sql(text(sql_inter_implicit), db.bind)

        check_path(data_path)
        project_df.to_csv(os.path.join(data_path, "project.csv"), index=False)
        freelancer_df.to_csv(os.path.join(data_path, "freelancer.csv"), index=False)
        inter_df.to_csv(os.path.join(data_path, "inter.csv"), index=False)
        inter_implicit_df.to_csv(os.path.join(data_path, "inter_implicit.csv"), index=False)

    except Exception as e:
        print(f"Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")

    finally:
        db.close()


def preprocess_data(
        data_path: str,
        output_path: str,
        n_components: int,
        embed: bool = False,
        similarity: Optional[str] = None
) -> Optional[Tuple[Union[np.ndarray, torch.Tensor], Union[np.ndarray, torch.Tensor]]]:
    """
    Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò

    Args:
        data_path (str): Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Í≤ΩÎ°ú
        output_path (str): Î™®Îç∏ Ï†ÄÏû• Í≤ΩÎ°ú
        n_components (int): ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© Î≤°ÌÑ∞Ïóê ÏÇ¨Ïö©Ìï† PCA Ï£ºÏÑ±Î∂Ñ Í∞úÏàò
        embed (bool): Ï†ÑÏ≤òÎ¶¨ Î∞©Ïãù. ÏûÑÎ≤†Îî©ÏùÑ ÏÇ¨Ïö©Ìï† Í≤ΩÏö∞ True. Í∏∞Î≥∏Í∞íÏùÄ False (Ïù∏ÏΩîÎî©)
        similarity (Optional[str]): Ïú†ÏÇ¨ÎèÑÎ•º Ï∂îÍ∞Ä ÌîºÏ≤òÎ°ú ÏÇ¨Ïö©Ìï† Í≤ΩÏö∞ Ï¢ÖÎ•ò ÏÑ†ÌÉù. Í∏∞Î≥∏Í∞íÏùÄ None ("cosine", "dot_product", "jaccard")


    Returns:
        Optional[Tuple[Union[np.ndarray, torch.Tensor], Union[np.ndarray, torch.Tensor]]]: similarityÎ•º ÏÑ†ÌÉùÌïòÎ©¥ Ïú†ÏÇ¨ÎèÑ Ï∂úÎ†•. Í∏∞Î≥∏Í∞íÏùÄ None
    """
    project_df = pd.read_csv(os.path.join(data_path, "project.csv"))
    freelancer_df = pd.read_csv(os.path.join(data_path, "freelancer.csv"))
    inter_df = pd.read_csv(os.path.join(data_path, "inter.csv"))

    print("üìç preprocessing project data ==============================")
    # ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© (Upstage Embeddings -> PCA)
    project_df = Preprocessing.text_embedding(project_df, "project_content", n_components, output_path)

    # Î≤îÏ£ºÌòï Î≥ÄÏàò Ïù∏ÏΩîÎî© (Î©ÄÌã∞-Ìï´)
    project_df = Preprocessing.encode_categorical_features(
        project_df,
        categorical_cols=["category_id", "skill_id"]
    )
    project_category_df = project_df.iloc[:, 6:16]
    project_skill_df = project_df.iloc[:, 16:]

    if embed:
        # Î≤îÏ£ºÌòï Î≥ÄÏàò ÏûÑÎ≤†Îî© (torch.nn.Embedding)
        project_category_df = Preprocessing.embed_categorical_features(
            project_category_df,
            num_features=project_category_df.shape[1],
            embedding_dim=16,
            name="project",
            feature="category",
        )
        project_skill_df = Preprocessing.embed_categorical_features(
            project_skill_df,
            num_features=project_skill_df.shape[1],
            embedding_dim=16,
            name="project",
            feature="skill",
        )

        # Í∏∞Ï°¥ Ïù∏ÏΩîÎî© Î≥ÄÏàò Ï†úÍ±∞ ÌõÑ ÏûÑÎ≤†Îî© Î≥ÄÏàò Ï∂îÍ∞Ä
        project_df = project_df.drop(columns=project_df.columns[6:])
        project_df = pd.concat([project_df, project_category_df, project_skill_df], axis=1)

    print("üìç preprocessing freelancer data ===========================")
    # Î≤îÏ£ºÌòï Î≥ÄÏàò Ïù∏ÏΩîÎî© (Î©ÄÌã∞-Ìï´)
    freelancer_df = Preprocessing.encode_categorical_features(
        freelancer_df,
        categorical_cols=["category_id", "skill_id"],
        skill_col="skill_id",
        expertise_col="skill_temp"
    )
    freelancer_category_df = freelancer_df.iloc[:, 3:13]
    freelancer_skill_df = freelancer_df.iloc[:, 13:]

    if embed:
        # Î≤îÏ£ºÌòï Î≥ÄÏàò ÏûÑÎ≤†Îî© (torch.nn.Embedding)
        freelancer_category_df = Preprocessing.embed_categorical_features(
            freelancer_category_df,
            num_features=freelancer_category_df.shape[1],
            embedding_dim=16,
            name="freelancer",
            feature="category",
        )
        freelancer_skill_df = Preprocessing.embed_categorical_features(
            freelancer_skill_df,
            num_features=freelancer_skill_df.shape[1],
            embedding_dim=16,
            name="freelancer",
            feature="skill",
            weight=True
        )

        # Í∏∞Ï°¥ Ïù∏ÏΩîÎî© Î≥ÄÏàò Ï†úÍ±∞ ÌõÑ ÏûÑÎ≤†Îî© Î≥ÄÏàò Ï∂îÍ∞Ä
        freelancer_df = freelancer_df.drop(columns=freelancer_df.columns[3:])
        freelancer_df = pd.concat([freelancer_df, freelancer_category_df, freelancer_skill_df], axis=1)

    project_df.to_csv(os.path.join(data_path, "project.csv"), index=False)
    freelancer_df.to_csv(os.path.join(data_path, "freelancer.csv"), index=False)

    # Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ (Ïù∏ÏΩîÎî©/ÏûÑÎ≤†Îî© Îëò Îã§ ÏÇ¨Ïö© Í∞ÄÎä•. Îã®, ÏûêÏπ¥Îìú Ïú†ÏÇ¨ÎèÑÎäî Ïù∏ÏΩîÎî©Îßå ÏÇ¨Ïö© Í∞ÄÎä•)
    if similarity:
        print(f"üìç calculating {similarity} similiarities =======================")

        category_similarity_df = Preprocessing.calculate_similarity_matrix(
            project_category_df,
            freelancer_category_df,
            method=similarity,
            batch_size=500
        )
        skill_similarity_df = Preprocessing.calculate_similarity_matrix(
            project_skill_df,
            freelancer_skill_df,
            method=similarity,
            batch_size=500
        )

        print(f"üìç Merge inter.csv and {similarity} similarity dataframes =======================")
        # inter_dfÏôÄ Ïú†ÏÇ¨ÎèÑ Îç∞Ïù¥ÌÑ∞Ïóê merge_index Ï∂îÍ∞Ä
        inter_df["merge_index"] = inter_df["project_id"].astype(str) + "_" + inter_df["freelancer_id"].astype(str)

        category_similarity_df["merge_index"] = project_df["project_id"].astype(str) + "_" + freelancer_df["freelancer_id"].astype(str)
        skill_similarity_df["merge_index"] = project_df["project_id"].astype(str) + "_" + freelancer_df["freelancer_id"].astype(str)

        # inter_df Í∏∞Ï§ÄÏúºÎ°ú Ïú†ÏÇ¨ÎèÑ Îç∞Ïù¥ÌÑ∞ ÌïÑÌÑ∞ÎßÅ
        category_similarity_df = category_similarity_df[category_similarity_df["merge_index"].isin(inter_df["merge_index"])]
        skill_similarity_df = skill_similarity_df[skill_similarity_df["merge_index"].isin(inter_df["merge_index"])]

        inter_df = inter_df.set_index("merge_index")
        category_similarity_df = category_similarity_df.set_index("merge_index")
        skill_similarity_df = skill_similarity_df.set_index("merge_index")

        # Ï≤≠ÌÅ¨ Îã®ÏúÑÎ°ú inter.csvÏóê Ïú†ÏÇ¨ÎèÑ Î≥ëÌï© (Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî Î¨∏Ï†ú)
        output_path = os.path.join(data_path, "inter.csv")

        chunk_size = 15000  # Ìïú Î≤àÏóê Ï≤òÎ¶¨Ìï† Ìñâ Í∞úÏàò
        total_chunks = len(inter_df) // chunk_size + (1 if len(inter_df) % chunk_size > 0 else 0)  # Ï†ÑÏ≤¥ Ï≤≠ÌÅ¨ Í∞úÏàò Í≥ÑÏÇ∞

        with open(output_path, "w") as f:
            with tqdm(total=total_chunks, desc="üîÑ Merging similarity data", unit="chunk") as pbar:
                for chunk_start in range(0, len(inter_df), chunk_size):
                    chunk_end = min(chunk_start + chunk_size, len(inter_df))
                    chunk = inter_df.iloc[chunk_start:chunk_end]

                    # Î∞∞Ïπò Îã®ÏúÑÎ°ú Ïú†ÏÇ¨ÎèÑ Î≥ëÌï©
                    chunk = chunk.merge(category_similarity_df, on="merge_index", how="left", suffixes=("", "_category"), sort=False).fillna(0.0)
                    chunk = chunk.merge(skill_similarity_df, on="merge_index", how="left", suffixes=("", "skill"), sort=False).fillna(0.0)

                    # Ï≤´ Î≤àÏß∏ Ï≤≠ÌÅ¨Îäî Ìó§Îçî Ìè¨Ìï®, Ïù¥ÌõÑÏóêÎäî Ìó§Îçî ÏóÜÏù¥ Ï†ÄÏû•
                    if chunk_start == 0:
                        chunk.to_csv(f, mode="w", index=True)
                    else:
                        chunk.to_csv(f, mode="a", index=True, header=False)

                    pbar.update(1)

                pbar.close()

        print(f"inter.csv saved successfully with {similarity} similarity! ==========")
